{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimal code example to run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warning: the imports take a long time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "Uncomment lines below to install everything needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install numpy\n",
    "\n",
    "#!pip install requests\n",
    "\n",
    "#!pip install Pillow\n",
    "\n",
    "#!pip install matplotlib\n",
    "\n",
    "#!pip install ipywidgets\n",
    "\n",
    "#!pip install torch==1.9.0\n",
    "#!pip install torchvision\n",
    "\n",
    "#!pip install pycocotools\n",
    "\n",
    "#!pip install scikit-image\n",
    "\n",
    "#!pip install transformers\n",
    "\n",
    "#!pip install pytorch_lightning\n",
    "\n",
    "#!pip install timm==0.4.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git clone https://github.com/facebookresearch/detr.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install -c pytorch pytorch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install cython scipy\n",
    "#!pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch==1.8.1+cu111 torchvision==0.9.1+cu111 torchaudio==0.8.1 -f https://download.pytorch.org/whl/lts/1.8/torch_lts.html\n",
    "#!pip install torchtext==0.9.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.models import resnet50\n",
    "import torchvision.transforms as T\n",
    "torch.set_grad_enabled(False);\n",
    "\n",
    "import os\n",
    "import torch.utils.data\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "import skimage.io as io\n",
    "import pylab\n",
    "pylab.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "import torchvision\n",
    "\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "\n",
    "from transformers import DetrFeatureExtractor, DetrConfig, DetrForObjectDetection, DetrModel, DetrPreTrainedModel\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0,\"/Users/apolline1/Documents/DTU/Courses/Semester1/minimal_example/detr/\")\n",
    "#print(sys.path)\n",
    "from datasets import get_coco_api_from_dataset\n",
    "from datasets.coco_eval import CocoEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini training\n",
    "\n",
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes=6\n",
    "batch_size = 1\n",
    "num_workers=0\n",
    "gpu=0\n",
    "max_epochs = 1\n",
    "fdr = False\n",
    "logit_threshold = 1.5\n",
    "p_threshold = 0.7\n",
    "lr = 1e-4 ; lr_backbone = 1e-5 ; weight_decay=1e-4\n",
    "gradient_clip_val = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Category Ids: [1, 2, 3, 4, 5]\n",
      "Categories: ['Person', 'Bike', 'Helmet', 'Phone', 'Airbag']\n"
     ]
    }
   ],
   "source": [
    "coco=COCO(\"content/content_train/trainJson.json\")\n",
    "print(\"Category Ids:\",coco.getCatIds())\n",
    "cats = coco.loadCats(coco.getCatIds())\n",
    "catnames=[cat['name'] for cat in cats]\n",
    "print('Categories: {}'.format(catnames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CocoDetection class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoDetection(torchvision.datasets.CocoDetection):\n",
    "    def __init__(self, img_folder, feature_extractor, train=True):\n",
    "        ann_file = os.path.join(img_folder, \"content_train/trainJson.json\" if train else \"content_test/testJson.json\")\n",
    "        img_folder = os.path.join(img_folder, \"content_train/trainData\" if train else \"content_test/testData\")\n",
    "        if os.path.lexists(os.path.join(img_folder, \".DS_Store\")): \n",
    "          os.remove(os.path.join(img_folder, \".DS_Store\"))\n",
    "        super(CocoDetection, self).__init__(img_folder, ann_file)\n",
    "        self.feature_extractor = feature_extractor\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # read in PIL image and target in COCO format\n",
    "        img, target = super(CocoDetection, self).__getitem__(idx)\n",
    "        \n",
    "        # preprocess image and target (converting target to DETR format, resizing + normalization of both image and target)\n",
    "        image_id = self.ids[idx]\n",
    "        target = {'image_id': image_id, 'annotations': target}\n",
    "        encoding = self.feature_extractor(images=img, annotations=target, return_tensors=\"pt\")\n",
    "        pixel_values = encoding[\"pixel_values\"].squeeze() # remove batch dimension\n",
    "        target = encoding[\"labels\"][0] # remove batch dimension\n",
    "\n",
    "        return pixel_values, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the data in COCO format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Number of training examples: 10\n",
      "Number of validation examples: 3\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = DetrFeatureExtractor.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "\n",
    "train_dataset = CocoDetection(img_folder= \"content/\", feature_extractor=feature_extractor)\n",
    "val_dataset = CocoDetection(img_folder= \"content/\", feature_extractor=feature_extractor, train=False)\n",
    "\n",
    "print(\"Number of training examples:\", len(train_dataset))\n",
    "print(\"Number of validation examples:\", len(val_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader objects for training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "  pixel_values = [item[0] for item in batch]\n",
    "  encoding = feature_extractor.pad_and_create_pixel_mask(pixel_values, return_tensors=\"pt\")\n",
    "  labels = [item[1] for item in batch]\n",
    "  batch = {}\n",
    "  batch['pixel_values'] = encoding['pixel_values']\n",
    "  batch['pixel_mask'] = encoding['pixel_mask']\n",
    "  batch['labels'] = labels\n",
    "  return batch\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, collate_fn=collate_fn, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
    "val_dataloader = DataLoader(val_dataset, collate_fn=collate_fn, batch_size=batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DETR object class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Detr(pl.LightningModule):\n",
    "\n",
    "     def __init__(self, lr, lr_backbone, weight_decay):\n",
    "         super().__init__()\n",
    "         # replace COCO classification head with custom head\n",
    "         self.model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\", \n",
    "                                                             num_labels=num_classes,\n",
    "                                                             ignore_mismatched_sizes=True)\n",
    "         # see https://github.com/PyTorchLightning/pytorch-lightning/pull/1896\n",
    "         # see https://github.com/huggingface/transformers/issues/12643 : ignore warnings\n",
    "         self.lr = lr\n",
    "         self.lr_backbone = lr_backbone\n",
    "         self.weight_decay = weight_decay\n",
    "\n",
    "     def forward(self, pixel_values, pixel_mask):\n",
    "       outputs = self.model(pixel_values=pixel_values, pixel_mask=pixel_mask)\n",
    "\n",
    "       return outputs\n",
    "     \n",
    "     def common_step(self, batch, batch_idx):\n",
    "       pixel_values = batch[\"pixel_values\"]\n",
    "       pixel_mask = batch[\"pixel_mask\"]\n",
    "       #labels = [{k: v.to(self.device) for k, v in t.items()} for t in batch[\"labels\"]]\n",
    "       labels = [{k: v for k, v in t.items()} for t in batch[\"labels\"]]\n",
    "\n",
    "       outputs = self.model(pixel_values=pixel_values, pixel_mask=pixel_mask, labels=labels)\n",
    "\n",
    "       loss = outputs.loss\n",
    "       loss_dict = outputs.loss_dict\n",
    "\n",
    "       return loss, loss_dict\n",
    "\n",
    "     def training_step(self, batch, batch_idx):\n",
    "        loss, loss_dict = self.common_step(batch, batch_idx)     \n",
    "        # logs metrics for each training_step,\n",
    "        # and the average across the epoch\n",
    "        self.log(\"training_loss\", loss)\n",
    "        for k,v in loss_dict.items():\n",
    "          self.log(\"train_\" + k, v.item())\n",
    "\n",
    "        return loss\n",
    "\n",
    "     def validation_step(self, batch, batch_idx):\n",
    "        loss, loss_dict = self.common_step(batch, batch_idx)     \n",
    "        self.log(\"validation_loss\", loss)\n",
    "        for k,v in loss_dict.items():\n",
    "          self.log(\"validation_\" + k, v.item())\n",
    "\n",
    "        return loss\n",
    "\n",
    "     def configure_optimizers(self):\n",
    "        param_dicts = [\n",
    "              {\"params\": [p for n, p in self.named_parameters() if \"backbone\" not in n and p.requires_grad]},\n",
    "              {\n",
    "                  \"params\": [p for n, p in self.named_parameters() if \"backbone\" in n and p.requires_grad],\n",
    "                  \"lr\": self.lr_backbone,\n",
    "              },\n",
    "        ]\n",
    "        optimizer = torch.optim.AdamW(param_dicts, lr=self.lr,\n",
    "                                  weight_decay=self.weight_decay)\n",
    "        \n",
    "        return optimizer\n",
    "\n",
    "     def train_dataloader(self):\n",
    "        return train_dataloader\n",
    "\n",
    "     def val_dataloader(self):\n",
    "        return val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DetrForObjectDetection were not initialized from the model checkpoint at facebook/detr-resnet-50 and are newly initialized because the shapes did not match:\n",
      "- class_labels_classifier.weight: found shape torch.Size([92, 256]) in the checkpoint and torch.Size([7, 256]) in the model instantiated\n",
      "- class_labels_classifier.bias: found shape torch.Size([92]) in the checkpoint and torch.Size([7]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = Detr(lr=lr, lr_backbone=lr_backbone, weight_decay=weight_decay)\n",
    "# ignore warning messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward pass on an item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['pixel_values', 'pixel_mask', 'labels'])\n"
     ]
    }
   ],
   "source": [
    "# Get an item to show before, during, after training\n",
    "batch = next(iter(val_dataloader))\n",
    "print(batch.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs logits shape torch.Size([1, 100, 7])\n"
     ]
    }
   ],
   "source": [
    "outputs = model(pixel_values=batch['pixel_values'], pixel_mask=batch['pixel_mask'])\n",
    "print(\"outputs logits shape\", outputs.logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation 0...\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      "IoU metric: bbox\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.004\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.009\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.005\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.005\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.200\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.200\n"
     ]
    }
   ],
   "source": [
    "base_ds = get_coco_api_from_dataset(val_dataset)\n",
    "iou_types = ['bbox']\n",
    "coco_evaluator = CocoEvaluator(base_ds, iou_types) # initialize evaluator with ground truths\n",
    "\n",
    "print(\"Running evaluation 0...\")\n",
    "for i_b, b in enumerate(val_dataloader):\n",
    "    # get the inputs\n",
    "    pixel_values = b[\"pixel_values\"]\n",
    "    pixel_mask = b[\"pixel_mask\"]\n",
    "    labels = [{k: v for k, v in t.items()} for t in b[\"labels\"]] # these are in DETR format, resized + normalized\n",
    "\n",
    "    # forward pass\n",
    "    outputs = model.model(pixel_values=pixel_values, pixel_mask=pixel_mask)\n",
    "\n",
    "    orig_target_sizes = torch.stack([target[\"orig_size\"] for target in labels], dim=0)\n",
    "    results = feature_extractor.post_process(outputs, orig_target_sizes) # convert outputs of model to COCO api\n",
    "    res = {target['image_id'].item(): output for target, output in zip(labels, results)}\n",
    "    coco_evaluator.update(res)\n",
    "\n",
    "coco_evaluator.synchronize_between_processes()\n",
    "coco_evaluator.accumulate()\n",
    "coco_evaluator.summarize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"-1.000\" is returned when no bounding box fits the criteria for the evaluation metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drawing annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = ['N/A', 'person', 'bike', 'helmet', 'phone', 'airbag', 'N/A']\n",
    "COLORS = ['red', 'green', 'blue', 'black', 'white', 'yellow']\n",
    "fnt = ImageFont.truetype(\"Pillow/Tests/fonts/Arial.ttf\", 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "img_id 1 orig_size tensor([1440, 1920]) size tensor([ 800, 1066])\n"
     ]
    }
   ],
   "source": [
    "bl = batch['labels'][0]\n",
    "img_id = bl['image_id'].item()\n",
    "orig_size = bl['orig_size']\n",
    "size = bl['size']\n",
    "print(\"\\n\\nimg_id\", img_id, \"orig_size\", orig_size, \"size\", size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'annotated_img1.png file created'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = train_dataset.coco.loadImgs(img_id)[0]\n",
    "annotations = train_dataset.coco.imgToAnns[img_id]\n",
    "\n",
    "def draw_annotations(image=image, img_id=img_id, annotations=annotations):\n",
    "  with Image.open(os.path.join(\"content/content_train/trainData/\", image['file_name'])) as im:\n",
    "    draw = ImageDraw.Draw(im, \"RGBA\")\n",
    "    for annotation in annotations:\n",
    "      box = annotation['bbox']\n",
    "      class_idx = annotation['category_id']\n",
    "      x,y,w,h = tuple(box)\n",
    "      draw.rectangle((x,y,x+w,y+h), outline=COLORS[class_idx-1], width=2)\n",
    "      draw.text((x, y), str(catnames[class_idx-1]), fill='white', font=fnt)\n",
    "    # save\n",
    "    im.save(\"annotated_img\"+str(img_id)+\".png\", \"PNG\")\n",
    "  return \"annotated_img\"+str(img_id)+\".png file created\"\n",
    "\n",
    "draw_annotations(image, img_id, annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drawing output annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils functions\n",
    "def box_cxcywh_to_xyxy(x):\n",
    "    x_c, y_c, w, h = x\n",
    "    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n",
    "        (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
    "    return b\n",
    "\n",
    "def rescale_bboxes(out_bbox, size):\n",
    "    img_w, img_h = size\n",
    "    b = np.array(box_cxcywh_to_xyxy(out_bbox))\n",
    "    b = np.multiply(b,np.array([img_w, img_h, img_w, img_h]))\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(img_id, pil_img, probs, boxes, train_nb):\n",
    "    draw = ImageDraw.Draw(pil_img, \"RGBA\")\n",
    "    colors = COLORS*20\n",
    "    for p, (xmin,ymin,xmax,ymax), c in zip(probs, boxes.tolist(), colors):\n",
    "      if max(p)>p_threshold-0.35: # had to lower threshold to have boxes to display \n",
    "        cl = p.argmax()\n",
    "        if (cl!=0 and cl!=6):\n",
    "          draw.rectangle((xmin,ymin,xmax,ymax), outline=colors[cl-1], width=2)\n",
    "          text_c = f'{CLASSES[cl]}: {p[cl]:0.2f}'\n",
    "          draw.text((xmin,ymin), text_c, fill='white', font=fnt)\n",
    "    pil_img.save('output'+str(train_nb)+'_img'+str(img_id)+'.png', \"PNG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probabilities\n",
    "logits = outputs.logits[0]\n",
    "softmax = nn.Softmax(dim=-1)\n",
    "probs = softmax(logits)\n",
    "\n",
    "# Boxes\n",
    "pred_boxes = outputs.pred_boxes[0]\n",
    "bsize=(orig_size[1],orig_size[0])\n",
    "boxes = np.array([rescale_bboxes(b,bsize) for b in pred_boxes])\n",
    "\n",
    "# Image\n",
    "pil_img = train_dataset.coco.loadImgs(img_id)[0]\n",
    "pil_img = Image.open(os.path.join(\"content/content_train/trainData/\", pil_img['file_name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(img_id, pil_img, probs, boxes, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "Missing logger folder: /Users/apolline1/Documents/DTU/Courses/Semester1/minimal_example/lightning_logs\n",
      "\n",
      "  | Name  | Type                   | Params\n",
      "-------------------------------------------------\n",
      "0 | model | DetrForObjectDetection | 41.5 M\n",
      "-------------------------------------------------\n",
      "41.3 M    Trainable params\n",
      "222 K     Non-trainable params\n",
      "41.5 M    Total params\n",
      "166.042   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:117: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "/opt/anaconda3/lib/python3.7/site-packages/pytorch_lightning/utilities/data.py:60: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  \"Trying to infer the `batch_size` from an ambiguous collection. The batch size we\"\n",
      "/opt/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:117: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "/opt/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:413: UserWarning: The number of training samples (10) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab516ac5633b43b099d1306ba5ccf575",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = Trainer(gpus=gpu, max_epochs=max_epochs, gradient_clip_val=gradient_clip_val, fast_dev_run=fdr)\n",
    "trainer.fit(model, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation after 1 training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation 1...\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      "IoU metric: bbox\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.089\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.279\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.089\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.056\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.122\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.122\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.122\n"
     ]
    }
   ],
   "source": [
    "base_ds = get_coco_api_from_dataset(val_dataset)\n",
    "iou_types = ['bbox']\n",
    "coco_evaluator = CocoEvaluator(base_ds, iou_types) # initialize evaluator with ground truths\n",
    "\n",
    "print(\"Running evaluation 1...\")\n",
    "for i_b, b in enumerate(val_dataloader):\n",
    "    # get the inputs\n",
    "    pixel_values = b[\"pixel_values\"]\n",
    "    pixel_mask = b[\"pixel_mask\"]\n",
    "    labels = [{k: v for k, v in t.items()} for t in b[\"labels\"]] # these are in DETR format, resized + normalized\n",
    "\n",
    "    # forward pass\n",
    "    outputs = model.model(pixel_values=pixel_values, pixel_mask=pixel_mask)\n",
    "\n",
    "    orig_target_sizes = torch.stack([target[\"orig_size\"] for target in labels], dim=0)\n",
    "    results = feature_extractor.post_process(outputs, orig_target_sizes) # convert outputs of model to COCO api\n",
    "    res = {target['image_id'].item(): output for target, output in zip(labels, results)}\n",
    "    coco_evaluator.update(res)\n",
    "\n",
    "coco_evaluator.synchronize_between_processes()\n",
    "coco_evaluator.accumulate()\n",
    "coco_evaluator.summarize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(val_dataloader))\n",
    "outputs = model(pixel_values=batch['pixel_values'], pixel_mask=batch['pixel_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probabilities\n",
    "logits = outputs.logits[0]\n",
    "softmax = nn.Softmax(dim=-1)\n",
    "probs = softmax(logits)\n",
    "\n",
    "# Boxes\n",
    "pred_boxes = outputs.pred_boxes[0]\n",
    "bsize=(orig_size[1],orig_size[0])\n",
    "boxes = np.array([rescale_bboxes(b.detach().numpy(),bsize) for b in pred_boxes])\n",
    "\n",
    "# Image\n",
    "pil_img = train_dataset.coco.loadImgs(img_id)[0]\n",
    "pil_img = Image.open(os.path.join(\"content/content_train/trainData/\", pil_img['file_name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(img_id, pil_img, probs, boxes, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
